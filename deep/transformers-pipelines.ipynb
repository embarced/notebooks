{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "transformers-pipelines.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/embarced/notebooks/blob/master/deep/transformers-pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "912d7d82"
      },
      "source": [
        "# Transformers: examples from basic pipeline tasks\n",
        "\n",
        "Examples taken from https://huggingface.co/transformers/task_summary.html"
      ],
      "id": "912d7d82"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhKtZIx7BC8S",
        "outputId": "d2b1dfdb-5442-4915-c728-ef288275a7bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "id": "BhKtZIx7BC8S",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.6.0'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCzzSt7XAuBn",
        "outputId": "43e81bbe-7247-4ac0-be61-a64bcffba6a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# when we are not training, we do not need a GPU\n",
        "!nvidia-smi"
      ],
      "id": "PCzzSt7XAuBn",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17bc5a59"
      },
      "source": [
        "# https://huggingface.co/transformers/installation.html\n",
        "!pip install -q transformers"
      ],
      "id": "17bc5a59",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKkytQc2Al0y",
        "outputId": "eeb020a1-cd29-43c4-c17d-38ad84032f02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import transformers\n",
        "transformers.__version__"
      ],
      "id": "UKkytQc2Al0y",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'4.9.2'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAmSIKEIEFiQ"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "id": "WAmSIKEIEFiQ",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78b74e90"
      },
      "source": [
        "# shows all possible tasks\n",
        "pipeline?"
      ],
      "id": "78b74e90",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI1-bxeAGDdf"
      },
      "source": [
        "## Sentiment Analysis\n",
        "\n",
        "model trained on the glue dataset: https://huggingface.co/datasets/glue\n"
      ],
      "id": "tI1-bxeAGDdf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87b53c5b",
        "outputId": "93f61936-f39b-414b-a19c-a74509e1745d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "classifier = pipeline(task=\"sentiment-analysis\")\n",
        "classifier.model.name_or_path"
      ],
      "id": "87b53c5b",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'distilbert-base-uncased-finetuned-sst-2-english'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp05bH77D5Py",
        "outputId": "09de28da-1362-4c02-e28a-cdaf65003ce5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classifier(\"I hate you\")"
      ],
      "id": "Gp05bH77D5Py",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_068zScPFOaV",
        "outputId": "96f78e79-9512-47d1-963d-268482409f54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "classifier(\"I love you\")"
      ],
      "id": "_068zScPFOaV",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEC-kpdLGLhk"
      },
      "source": [
        "## Are two sequences paraphrases of each other?"
      ],
      "id": "ZEC-kpdLGLhk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxj990PqGBGO",
        "outputId": "c261b6d2-5e36-41af-a570-d6be64d497cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased-finetuned-mrpc\")\n",
        "\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "\n",
        "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
        "sequence_1 = \"Apples are especially bad for your health\"\n",
        "sequence_2 = \"HuggingFace's headquarters are situated in Manhattan\"\n",
        "\n",
        "# The tokekenizer will automatically add any model specific separators (i.e. <CLS> and <SEP>) and tokens to the sequence, as well as compute the attention masks.\n",
        "paraphrase = tokenizer(sequence_0, sequence_2, return_tensors=\"tf\")\n",
        "not_paraphrase = tokenizer(sequence_0, sequence_1, return_tensors=\"tf\")\n",
        "\n",
        "paraphrase_classification_logits = model(paraphrase)[0]\n",
        "not_paraphrase_classification_logits = model(not_paraphrase)[0]\n",
        "\n",
        "paraphrase_results = tf.nn.softmax(paraphrase_classification_logits, axis=1).numpy()[0]\n",
        "not_paraphrase_results = tf.nn.softmax(not_paraphrase_classification_logits, axis=1).numpy()[0]\n",
        "\n",
        "# Should be paraphrase\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {int(round(paraphrase_results[i] * 100))}%\")\n",
        "\n",
        "# Should not be paraphrase\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {int(round(not_paraphrase_results[i] * 100))}%\")"
      ],
      "id": "gxj990PqGBGO",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-cased-finetuned-mrpc were not used when initializing TFBertForSequenceClassification: ['dropout_183']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bert-base-cased-finetuned-mrpc.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "not paraphrase: 10%\n",
            "is paraphrase: 90%\n",
            "not paraphrase: 94%\n",
            "is paraphrase: 6%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVvkFYT1Pxj7"
      },
      "source": [
        "## Extractive Question Answering\n",
        "\n",
        "https://huggingface.co/transformers/task_summary.html#extractive-question-answering"
      ],
      "id": "NVvkFYT1Pxj7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_e-WImAKPEf",
        "outputId": "f3b71ad4-fffa-4676-c264-c17bf7b0080b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer.model.name_or_path"
      ],
      "id": "E_e-WImAKPEf",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'distilbert-base-cased-distilled-squad'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEPSDgg1PVMR",
        "outputId": "fccf7e2e-8007-4124-e0e4-a0d6dfb09941",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
        "\"\"\"\n",
        "\n",
        "result = question_answerer(question=\"What is extractive question answering?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\n",
        "\n",
        "result = question_answerer(question=\"What is a good example of a question answering dataset?\", context=context)\n",
        "print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")"
      ],
      "id": "bEPSDgg1PVMR",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answer: 'the task of extracting an answer from a text given a question', score: 0.6177, start: 34, end: 95\n",
            "Answer: 'SQuAD dataset', score: 0.5152, start: 147, end: 160\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTCXwNcXBxnh"
      },
      "source": [
        "# German \n",
        "\n",
        "Models and data sets are rare unfortunately\n",
        "\n",
        "Models\n",
        "* https://huggingface.co/bert-base-german-cased\n",
        "\n",
        "Data Sets\n",
        "* https://tblock.github.io/10kGNAD/"
      ],
      "id": "rTCXwNcXBxnh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5d0ba34",
        "outputId": "3fd8e36f-f87e-4f8e-9f02-df448601839d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import TFAutoModelWithLMHead, AutoTokenizer\n",
        "import tensorflow as tf\n",
        "\n",
        "# model_name = \"distilbert-base-cased\"\n",
        "model_name = \"bert-base-german-cased\"\n",
        "# works with Pytorch only\n",
        "# model_name = \"bert-base-german-dbmdz-cased\"\n",
        "# model_name = \"bert-base-multilingual-cased\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = TFAutoModelWithLMHead.from_pretrained(model_name)\n",
        "\n",
        "# sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "sequence = f\"Deutschland ist ein tolles Land. Als Bürger hast du das Recht {tokenizer.mask_token} zu tun was nicht gegen das Gesetz ist.\"\n",
        "\n",
        "input = tokenizer.encode(sequence, return_tensors=\"tf\")\n",
        "mask_token_index = tf.where(input == tokenizer.mask_token_id)[0, 1]\n",
        "\n",
        "token_logits = model(input)[0]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "top_5_tokens = tf.math.top_k(mask_token_logits, 5).indices.numpy()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "id": "f5d0ba34",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_tf_auto.py:593: FutureWarning: The class `TFAutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `TFAutoModelForCausalLM` for causal language models, `TFAutoModelForMaskedLM` for masked language models and `TFAutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht etwas zu tun was nicht gegen das Gesetz ist.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht [unused_punctuation1] zu tun was nicht gegen das Gesetz ist.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht alles zu tun was nicht gegen das Gesetz ist.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht nichts zu tun was nicht gegen das Gesetz ist.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht so zu tun was nicht gegen das Gesetz ist.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DSNh_BUBT0Y",
        "outputId": "dbb40695-6660-4834-b4bc-dbf1826f27fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"bert-base-german-dbmdz-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelWithLMHead.from_pretrained(model_name)\n",
        "\n",
        "# sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "sequence = f\"Deutschland ist ein tolles Land. Als Bürger hast du das Recht {tokenizer.mask_token} zu tun.\"\n",
        "\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "\n",
        "token_logits = model(input)[0]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "id": "-DSNh_BUBT0Y",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:902: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of the model checkpoint at bert-base-german-dbmdz-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht nichts zu tun.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht etwas zu tun.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht das zu tun.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht was zu tun.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht , zu tun.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLjSv1k4Qr14",
        "outputId": "5a33da41-bf57-4391-ce33-7eeae91b9847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"bert-base-multilingual-cased\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelWithLMHead.from_pretrained(model_name)\n",
        "\n",
        "# sequence = f\"Distilled models are smaller than the models they mimic. Using them instead of the large versions would help {tokenizer.mask_token} our carbon footprint.\"\n",
        "sequence = f\"Deutschland ist ein tolles Land. Als Bürger hast du das Recht {tokenizer.mask_token} zu tun.\"\n",
        "\n",
        "input = tokenizer.encode(sequence, return_tensors=\"pt\")\n",
        "mask_token_index = torch.where(input == tokenizer.mask_token_id)[1]\n",
        "\n",
        "token_logits = model(input)[0]\n",
        "mask_token_logits = token_logits[0, mask_token_index, :]\n",
        "\n",
        "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
        "\n",
        "for token in top_5_tokens:\n",
        "    print(sequence.replace(tokenizer.mask_token, tokenizer.decode([token])))"
      ],
      "id": "hLjSv1k4Qr14",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:902: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht Deutschland zu tun.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht , zu tun.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht Deutsch zu tun.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht zu zu tun.\n",
            "Deutschland ist ein tolles Land. Als Bürger hast du das Recht Freiheit zu tun.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaK8tegdROT5"
      },
      "source": [
        ""
      ],
      "id": "RaK8tegdROT5",
      "execution_count": 15,
      "outputs": []
    }
  ]
}