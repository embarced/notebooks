{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdfXfXK2-CiG"
   },
   "source": [
    "# Creating a reproducible pipeline using TFX\n",
    "\n",
    "![](https://www.tensorflow.org/tfx/guide/images/prog_fin.png)\n",
    "          \n",
    "\n",
    "## Story\n",
    "1. To make work reproducible we define the workflow as a TFX pipeline\n",
    "1. Additionally we generate statistics from the data we use for training\n",
    "1. After training, before pushing to next stage (possibly integration or production) we execute a couple of tests working as binary predictates\n",
    "   * Tesla does this with their models for autonomous driving: https://youtu.be/hx7BXih7zx8?t=776\n",
    "   * Could be like: \n",
    "     * for well known inputs we expect outputs known to be correct within a certain margin\n",
    "     * certain scores need to be matched\n",
    "     \n",
    "\n",
    "## Resources     \n",
    "Parts copied from https://www.tensorflow.org/tfx/guide#tfx_standard_components: \n",
    "1. **Basics:** https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/penguin_simple.ipynb\n",
    "  * ExampleGen is the initial input component of a pipeline that ingests and optionally splits the input dataset: https://www.tensorflow.org/tfx/guide/examplegen\n",
    "  * Trainer trains the model: https://www.tensorflow.org/tfx/guide/trainer\n",
    "  * Pusher deploys the model on a serving infrastructure: https://www.tensorflow.org/tfx/guide/pusher\n",
    "1. **Data validation:** https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/penguin_tfdv.ipynb\n",
    "  * https://www.tensorflow.org/tfx/guide/tfdv\n",
    "  * StatisticsGen calculates statistics for the dataset: https://www.tensorflow.org/tfx/guide/statsgen\n",
    "  * SchemaGen examines the statistics and creates a data schema: https://www.tensorflow.org/tfx/guide/schemagen\n",
    "  * ExampleValidator looks for anomalies and missing values in the dataset: https://www.tensorflow.org/tfx/guide/exampleval\n",
    "1. **Feature Engineering:** https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/penguin_tft.ipynb\n",
    "  * https://www.tensorflow.org/tfx/guide/tft\n",
    "  * Transform performs feature engineering on the dataset: https://www.tensorflow.org/tfx/guide/transform\n",
    "1. **Model Analysis:** https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/penguin_tfma.ipynb\n",
    "  * https://www.tensorflow.org/tfx/guide/tfma\n",
    "  * Evaluator performs deep analysis of the training results and helps you validate your exported models, ensuring that they are \"good enough\" to be pushed to production: https://www.tensorflow.org/tfx/guide/evaluator\n",
    "\n",
    "https://www.tensorflow.org/tfx/tutorials\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSzUEiEPgpuM",
    "outputId": "2ca035f1-c786-42e5-d2b6-b0876c9d0995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.6.0\n",
      "TFX version: 1.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "i3OoEXQtg-c_"
   },
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "\n",
    "# logging.set_verbosity(logging.INFO)\n",
    "logging.set_verbosity(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xYpianTTk8Rp"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Hd9fZ3gOhEs1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir {DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVMIRHNShGR6",
    "outputId": "604531f2-2aec-49ed-9e48-e9955b4bf8e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 54421  100 54421    0     0   103k      0 --:--:-- --:--:-- --:--:--  102k\n"
     ]
    }
   ],
   "source": [
    "!curl -o {DATA_DIR}/data.csv https://raw.githubusercontent.com/embarced/notebooks/master/mlops/insurance-customers-risk-1500.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1U-Fmy9Xj9UY",
    "outputId": "acc05580-a75e-4258-82a3-78e4ce00db24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 112\r\n",
      "-rw-r--r-- 1 olli olli 54421 Nov 11 11:28 data.csv\r\n",
      "-rw-r--r-- 1 olli olli 54500 Nov  7 12:45 drifted-data.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMovZLm5JMJq",
    "outputId": "e9a21f4b-a040-4ffc-a09a-b884a2482178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed,age,miles,group,risk\r\n",
      "97.0,44.0,30.0,1,0.5976112279191053\r\n",
      "135.0,63.0,29.0,1,0.4527103520003165\r\n",
      "111.0,26.0,34.0,0,0.750233962021037\r\n",
      "97.0,25.0,10.0,1,0.32524900971290915\r\n",
      "114.0,38.0,22.0,2,0.26973096398543817\r\n",
      "130.0,55.0,34.0,0,0.5871633471963134\r\n",
      "118.0,40.0,51.0,0,0.8753213424169751\r\n",
      "143.0,42.0,34.0,2,0.23665405507569381\r\n",
      "110.0,43.0,31.0,2,0.0\r\n"
     ]
    }
   ],
   "source": [
    "!head {DATA_DIR}/data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTy4Ubzo2ZtX",
    "outputId": "1caad6ac-19a7-42ce-ae18-51c5fa71fec3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18916\r\n",
      "-rw-r--r--  1 olli olli 1425143 Nov  5 12:27 1_mlops_train.ipynb\r\n",
      "-rw-r--r--  1 olli olli 1819071 Oct 28 14:03 1_mlops_train_nsl.ipynb\r\n",
      "-rw-r--r--  1 olli olli  223174 Nov 11 11:26 2_mlops_serve.ipynb\r\n",
      "-rw-r--r--  1 olli olli 1695449 Nov  6 11:39 3_mlops_shift.ipynb\r\n",
      "-rw-r--r--  1 olli olli   47508 Nov 11 11:28 4_mlops_tfx_training.ipynb\r\n",
      "-rw-r--r--  1 olli olli   43870 Nov  7 12:46 5_mlops_tfdv_skew.ipynb\r\n",
      "drwxr-xr-x  2 olli olli    4096 Nov  2 11:22 __pycache__\r\n",
      "drwxr-xr-x  4 olli olli    4096 Nov  5 12:24 classifier\r\n",
      "drwxr-xr-x  4 olli olli    4096 Nov  5 14:01 classifier-pruned\r\n",
      "-rw-r--r--  1 olli olli 1207767 Nov  5 14:03 classifier-pruned.tgz\r\n",
      "-rw-r--r--  1 olli olli 3130024 Nov  6 10:08 classifier.h5\r\n",
      "-rw-r--r--  1 olli olli 5714201 Nov  5 12:24 classifier.tgz\r\n",
      "drwxr-xr-x  2 olli olli    4096 Oct 27 16:09 data\r\n",
      "drwxr-xr-x  2 olli olli    4096 Oct 27 16:12 drifted_data\r\n",
      "-rw-r--r--  1 olli olli 2101066 Nov  7 12:01 generate.ipynb\r\n",
      "drwxr-xr-x  3 olli olli    4096 Oct 24 18:42 insurance\r\n",
      "-rw-r--r--  1 olli olli   54500 Aug 14 20:29 insurance-customers-risk-1500-shift.csv\r\n",
      "-rw-r--r--  1 olli olli   54435 Aug 14 20:29 insurance-customers-risk-1500-test.csv\r\n",
      "-rw-r--r--  1 olli olli   54421 Aug 14 20:29 insurance-customers-risk-1500.csv\r\n",
      "-rw-r--r--  1 olli olli  344064 Nov  2 11:22 metadata.db\r\n",
      "-rw-r--r--  1 olli olli   40766 Oct 27 16:58 mlops_tfx_playground.ipynb\r\n",
      "drwxr-xr-x 11 olli olli    4096 Nov  2 11:22 model\r\n",
      "-rw-r--r--  1 olli olli   95799 Nov  5 12:44 model.png\r\n",
      "drwxr-xr-x  2 olli olli    4096 Oct 27 16:11 original_data\r\n",
      "drwxr-xr-x 10 olli olli    4096 Oct 29 15:33 pipeline\r\n",
      "drwxr-xr-x  2 olli olli    4096 Oct 27 16:56 schema\r\n",
      "drwxr-xr-x  5 olli olli    4096 Oct 26 18:19 schema-pipeline\r\n",
      "drwxr-xr-x  5 olli olli    4096 Oct 26 18:19 schema_pipeline\r\n",
      "-rw-r--r--  1 olli olli    3168 Nov  7 10:17 server.log\r\n",
      "drwxr-xr-x  2 olli olli    4096 Oct 27 16:56 stats\r\n",
      "-rw-r--r--  1 olli olli 1225551 Aug 14 20:29 train.ipynb\r\n",
      "-rw-r--r--  1 olli olli    3928 Nov  2 11:22 trainer.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-2-G4zPw2oGq"
   },
   "outputs": [],
   "source": [
    "_trainer_module_file = 'trainer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOUodcze2qUK",
    "outputId": "18a6c87a-e03e-40f2-e77e-90fd2e8cb1ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_trainer_module_file}\n",
    "\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Dropout, \\\n",
    "                                    BatchNormalization, Activation,\\\n",
    "                                    Input, concatenate\n",
    "from typing import List\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "\n",
    "_FEATURE_KEYS = ['age', 'speed']\n",
    "_LABEL_KEY = 'group'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 32\n",
    "_EVAL_BATCH_SIZE = 32\n",
    "\n",
    "# Since we're not generating or creating a schema, we will instead create\n",
    "# a feature spec.  Since there are a fairly small number of features this is\n",
    "# manageable for this dataset.\n",
    "_FEATURE_SPEC = {\n",
    "    **{\n",
    "        feature: tf.io.FixedLenFeature(shape=[1], dtype=tf.float32)\n",
    "           for feature in _FEATURE_KEYS\n",
    "       },\n",
    "    _LABEL_KEY: tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
    "}\n",
    "\n",
    "num_features = len(_FEATURE_KEYS)\n",
    "dropout = 0.5\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              schema: schema_pb2.Schema,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    schema: schema of the input data.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
    "      schema=schema).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "\n",
    "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = Dense(500)(d)\n",
    "    d = Activation('relu')(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = Dropout(dropout)(d)\n",
    "\n",
    "  outputs = Dense(name='output', units=3, activation='softmax')(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "\n",
    "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
    "  # version provided by pipeline author. A schema can also derived from TFT\n",
    "  # graph if a Transform component is used. In the case when either is missing,\n",
    "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
    "  # feature_spec, but the schema returned would be very primitive.\n",
    "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      schema,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  # The result of the training should be saved in `fn_args.serving_model_dir`\n",
    "  # directory.\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rXeTQ4ExiHf_"
   },
   "outputs": [],
   "source": [
    "# !cat trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "sqOMX7EU7-Ec"
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "def _create_pipeline(pipeline_name: str, pipeline_root: str, data_root: str,\n",
    "                     module_file: str, serving_model_dir: str,\n",
    "                     metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a three component penguin pipeline with TFX.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "\n",
    "  # Computes statistics over data for visualization and example validation.\n",
    "  statistics_gen = tfx.components.StatisticsGen(\n",
    "      examples=example_gen.outputs['examples'])\n",
    "\n",
    "  # Uses user-provided Python function that trains a model.\n",
    "  trainer = tfx.components.Trainer(\n",
    "      module_file=module_file,\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      train_args=tfx.proto.TrainArgs(num_steps=5000),\n",
    "      eval_args=tfx.proto.EvalArgs(num_steps=15))\n",
    "\n",
    "  #Get the latest blessed model for Evaluator.\n",
    "  model_resolver = tfx.dsl.Resolver(\n",
    "      strategy_class=tfx.dsl.experimental.LatestBlessedModelStrategy,\n",
    "      model=tfx.dsl.Channel(type=tfx.types.standard_artifacts.Model),\n",
    "      model_blessing=tfx.dsl.Channel(\n",
    "          type=tfx.types.standard_artifacts.ModelBlessing)).with_id(\n",
    "              'latest_blessed_model_resolver')\n",
    "\n",
    "  #Uses TFMA to compute evaluation statistics over features of a model and\n",
    "  #   perform quality validation of a candidate model (compared to a baseline).\n",
    "\n",
    "  eval_config = tfma.EvalConfig(\n",
    "      model_specs=[tfma.ModelSpec(label_key='group')],\n",
    "      slicing_specs=[\n",
    "          # An empty slice spec means the overall slice, i.e. the whole dataset.\n",
    "          tfma.SlicingSpec(),\n",
    "          # Calculate metrics for each risk group\n",
    "          tfma.SlicingSpec(feature_keys=['group']),\n",
    "          ],\n",
    "      metrics_specs=[\n",
    "          tfma.MetricsSpec(per_slice_thresholds={\n",
    "              'sparse_categorical_accuracy':\n",
    "                  tfma.PerSliceMetricThresholds(thresholds=[\n",
    "                      tfma.PerSliceMetricThreshold(\n",
    "                          slicing_specs=[tfma.SlicingSpec()],\n",
    "                          threshold=tfma.MetricThreshold(\n",
    "                              value_threshold=tfma.GenericValueThreshold(\n",
    "                                   lower_bound={'value': 0.7}),\n",
    "                              # Change threshold will be ignored if there is no\n",
    "                              # baseline model resolved from MLMD (first run).\n",
    "                              change_threshold=tfma.GenericChangeThreshold(\n",
    "                                  direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
    "                                  absolute={'value': 0.1}))\n",
    "                       )]),\n",
    "          })],\n",
    "      )\n",
    "  evaluator = tfx.components.Evaluator(\n",
    "      examples=example_gen.outputs['examples'],\n",
    "      model=trainer.outputs['model'],\n",
    "      baseline_model=model_resolver.outputs['model'],\n",
    "      eval_config=eval_config)\n",
    "\n",
    "  # Checks whether the model passed the validation steps and pushes the model\n",
    "  # to a file destination if check passed.\n",
    "  pusher = tfx.components.Pusher(\n",
    "      model=trainer.outputs['model'],\n",
    "      model_blessing=evaluator.outputs['blessing'], # Pass an evaluation result.\n",
    "      push_destination=tfx.proto.PushDestination(\n",
    "          filesystem=tfx.proto.PushDestination.Filesystem(\n",
    "              base_directory=serving_model_dir)))\n",
    "\n",
    "  components = [\n",
    "      example_gen,\n",
    "      statistics_gen,\n",
    "      trainer,\n",
    "\n",
    "      # Following two components were added to the pipeline.\n",
    "      model_resolver,\n",
    "      evaluator,\n",
    "\n",
    "      pusher,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjHiypa7_HJu",
    "outputId": "410017fe-64c3-4826-afb8-87fc809ad3d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 19s 4ms/step - loss: 0.8248 - sparse_categorical_accuracy: 0.6407 - val_loss: 0.6827 - val_sparse_categorical_accuracy: 0.7104\n",
      "INFO:tensorflow:Assets written to: pipeline/Trainer/model/144/Format-Serving/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: pipeline/Trainer/model/144/Format-Serving/assets\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.2 s, sys: 13.7 s, total: 51 s\n",
      "Wall time: 33.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "pipeline = _create_pipeline(\n",
    "      pipeline_name='insurance-basic',\n",
    "      pipeline_root='pipeline',\n",
    "      data_root=DATA_DIR,\n",
    "      module_file=_trainer_module_file,\n",
    "      serving_model_dir='model',\n",
    "      metadata_path='metadata.db')\n",
    "\n",
    "tfx.orchestration.LocalDagRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLowvaLufwC4",
    "outputId": "38c36bcb-40a5-4f25-c4c1-1fe47cff3fa4"
   },
   "outputs": [],
   "source": [
    "from ml_metadata.proto import metadata_store_pb2\n",
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.portable.mlmd import execution_lib\n",
    "\n",
    "# TODO(b/171447278): Move these functions into the TFX library.\n",
    "\n",
    "def get_latest_artifacts(metadata, pipeline_name, component_id):\n",
    "  \"\"\"Output artifacts of the latest run of the component.\"\"\"\n",
    "  context = metadata.store.get_context_by_type_and_name(\n",
    "      'node', f'{pipeline_name}.{component_id}')\n",
    "  executions = metadata.store.get_executions_by_context(context.id)\n",
    "  latest_execution = max(executions,\n",
    "                         key=lambda e:e.last_update_time_since_epoch)\n",
    "  return execution_lib.get_artifacts_dict(metadata, latest_execution.id, \n",
    "                                          metadata_store_pb2.Event.OUTPUT)\n",
    "\n",
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.experimental.interactive import visualizations\n",
    "\n",
    "def visualize_artifacts(artifacts):\n",
    "  \"\"\"Visualizes artifacts using standard visualization modules.\"\"\"\n",
    "  for artifact in artifacts:\n",
    "    visualization = visualizations.get_registry().get_visualization(\n",
    "        artifact.type_name)\n",
    "    if visualization:\n",
    "      visualization.display(artifact)\n",
    "\n",
    "from tfx.orchestration.experimental.interactive import standard_visualizations\n",
    "standard_visualizations.register_standard_visualizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.metadata import Metadata\n",
    "from tfx.types import standard_component_specs\n",
    "\n",
    "metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\"metadata.db\")\n",
    "\n",
    "with Metadata(metadata_connection_config) as metadata_handler:\n",
    "  # Find output artifacts from MLMD.\n",
    "  example_gen_output = get_latest_artifacts(metadata_handler, \"insurance-basic\", 'CsvExampleGen')\n",
    "  example_artifacts = example_gen_output[standard_component_specs.EXAMPLES_KEY]\n",
    "\n",
    "  stat_gen_output = get_latest_artifacts(metadata_handler, \"insurance-basic\", 'StatisticsGen')\n",
    "  stats_artifacts = stat_gen_output[standard_component_specs.STATISTICS_KEY]\n",
    "\n",
    "  evaluator_output = get_latest_artifacts(metadata_handler, \"insurance-basic\", 'Evaluator')\n",
    "  eval_artifact = evaluator_output[standard_component_specs.EVALUATION_KEY][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pipeline/StatisticsGen/statistics/143'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_artifacts[0].uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "drwxr-xr-x 2 olli olli 4096 Nov 11 11:43 Split-eval\r\n",
      "drwxr-xr-x 2 olli olli 4096 Nov 11 11:43 Split-train\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l {stats_artifacts[0].uri}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘stats’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp {stats_artifacts[0].uri}/Split-train/FeatureStats.pb stats/TrainFeatureStats.pb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_artifacts(stats_artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 8\r\n",
      "drwxr-xr-x 12 olli olli 4096 Nov 11 11:36 blessing\r\n",
      "drwxr-xr-x 12 olli olli 4096 Nov 11 11:36 evaluation\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l pipeline/Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 40\r\n",
      "drwxr-xr-x 2 olli olli 4096 Oct 29 16:00 103\r\n",
      "drwxr-xr-x 2 olli olli 4096 Oct 29 16:03 109\r\n",
      "drwxr-xr-x 2 olli olli 4096 Nov  2 11:22 115\r\n",
      "drwxr-xr-x 2 olli olli 4096 Nov 11 11:29 121\r\n",
      "drwxr-xr-x 2 olli olli 4096 Nov 11 11:32 127\r\n",
      "drwxr-xr-x 2 olli olli 4096 Nov 11 11:33 133\r\n",
      "drwxr-xr-x 2 olli olli 4096 Nov 11 11:37 139\r\n",
      "drwxr-xr-x 2 olli olli 4096 Oct 29 15:38 85\r\n",
      "drwxr-xr-x 2 olli olli 4096 Oct 29 15:45 91\r\n",
      "drwxr-xr-x 2 olli olli 4096 Oct 29 15:53 97\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l pipeline/Evaluator/blessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "-rw-r--r-- 1 olli olli 0 Nov 11 11:37 NOT_BLESSED\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l pipeline/Evaluator/blessing/139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 olli olli 105 Oct 29 15:45 pipeline/Evaluator/evaluation/91/plots\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l pipeline/Evaluator/evaluation/91/plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\r\n",
      "drwxr-xr-x 4 olli olli 4096 Oct 26 17:17 1635261467\r\n",
      "drwxr-xr-x 4 olli olli 4096 Oct 26 17:32 1635262335\r\n",
      "drwxr-xr-x 4 olli olli 4096 Oct 26 17:46 1635263194\r\n",
      "drwxr-xr-x 4 olli olli 4096 Oct 27 15:57 1635343033\r\n",
      "drwxr-xr-x 4 olli olli 4096 Oct 27 16:15 1635344127\r\n",
      "drwxr-xr-x 4 olli olli 4096 Oct 27 16:49 1635346140\r\n",
      "drwxr-xr-x 4 olli olli 4096 Oct 29 15:38 1635514703\r\n",
      "drwxr-xr-x 4 olli olli 4096 Oct 29 15:45 1635515159\r\n",
      "drwxr-xr-x 4 olli olli 4096 Nov  2 11:22 1635848567\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326047cc6c1045139a46b4a92b9a0451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'group:0', 'metrics':…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://www.tensorflow.org/tfx/tutorials/model_analysis/tfma_basic#rendering_metrics\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "eval_result = tfma.load_eval_result(eval_artifact.uri)\n",
    "tfma.view.render_slicing_metrics(eval_result, slicing_column='group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "4-mlops-tfx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
